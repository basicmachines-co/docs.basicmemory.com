---
title: Semantic Search
description: Meaning-based retrieval with vector and hybrid search in Basic Memory.
---

When semantic dependencies are installed, Basic Memory automatically uses hybrid search — combining keyword matching with meaning-based retrieval. Search for "login security improvements" and find notes about "authentication hardening" even though the exact words don't match. Without semantic deps, search falls back to keyword-only mode.

::mermaid
---
code: |
  flowchart LR
      Q[Query] --> T[Text Search]
      Q --> V[Vector Search]
      T --> RRF[Rank Fusion]
      V --> RRF
      RRF --> R[Results]
---
::

---

## Search modes

Basic Memory supports three search modes. You can use them through conversation or through the `search_notes` tool directly.

### `text`

Keyword search. You type words, it finds notes containing those words. Supports boolean operators like `AND` and `OR`.

```python
search_notes(query="project AND planning", search_type="text")
```

Best when you know the exact terms — names, identifiers, tags.

### `vector`

Meaning-based search. Your query is turned into a vector and compared against all indexed content. Results are ranked by how similar the meaning is, not whether the exact words match.

```python
search_notes(query="how to speed up the app", search_type="vector")
```

Best for conceptual queries. Searching for "ways to make the frontend faster" finds notes about "React performance optimization" and "bundle size reduction."

### `hybrid` (default)

Runs both text and vector search, then combines results. Items found by both methods get boosted. Items found by only one still appear but rank lower.

```python
search_notes(query="authentication security", search_type="hybrid")
```

This is the default when semantic dependencies are installed. You get keyword precision plus semantic recall. If semantic deps aren't installed, search falls back to `text` mode automatically.

### When to use which

| Mode | Best for | Example query |
|------|----------|---------------|
| `text` | Exact terms, boolean filters, specific names | `"project AND planning"`, `"Ada Lovelace"` |
| `vector` | Conceptual discovery, paraphrase matching | `"how we handle failures gracefully"` |
| `hybrid` | General-purpose, everyday queries | `"authentication security"` |

Start with `hybrid`. Fall back to `text` for exact matches, and use `vector` when you're exploring and aren't sure what words were used.

---

## Enabling semantic search

Semantic search is enabled by default in Basic Memory, but the embedding dependencies are optional. If you installed Basic Memory without them, you'll need to add them.

### macOS (Homebrew)

Homebrew installs include semantic dependencies automatically — nothing extra to do.

### All platforms (uvx)

Run Basic Memory with semantic extras directly:

```bash
uvx 'basic-memory[semantic]' mcp
```

This is the quickest way to get going — `uvx` handles the install and runs the MCP server in one step.

### Alternative: install with uv

If you prefer a persistent install:

```bash
uv tool install 'basic-memory[semantic]'
```

Or upgrade an existing install to add semantic extras:

```bash
uv tool upgrade 'basic-memory[semantic]'
```

### Build embeddings

After installing, build the initial embeddings for your existing notes:

```bash
bm reindex --embeddings
```

This processes every note, breaks it into chunks, and generates vector embeddings. For a few hundred notes, expect 1–3 minutes with the local embedding model. After that, new and edited notes are embedded incrementally during normal sync.

### Platform compatibility

| Platform | FastEmbed (local) | OpenAI (API) |
|---|---|---|
| macOS ARM64 (Apple Silicon) | Yes | Yes |
| macOS x86_64 (Intel Mac) | No (see workaround) | Yes |
| Linux x86_64 | Yes | Yes |
| Linux ARM64 | Yes | Yes |
| Windows x86_64 | Yes | Yes |

### Intel Mac workaround

The default local embedding model doesn't run on Intel Macs. You have two options:

**Option 1: Use OpenAI embeddings (recommended)**

Requires an [OpenAI API subscription](https://platform.openai.com/api-keys) — create a key there and set it as an environment variable.

```bash
uv tool install basic-memory
uv pip install openai sqlite-vec
export BASIC_MEMORY_SEMANTIC_EMBEDDING_PROVIDER=openai
export OPENAI_API_KEY=sk-...
bm reindex --embeddings
```

**Option 2: Pin ONNX Runtime**

```bash
uv tool install 'basic-memory[semantic]'
uv pip install 'onnxruntime<1.24'
```

---

## How it works

### Chunking

Notes are not embedded as whole documents. Basic Memory breaks each note into smaller pieces before generating embeddings, so search can surface the specific *part* of a note that's relevant.

::mermaid
---
code: |
  flowchart TD
      N[Note] --> H[Headers]
      N --> O[Observations]
      N --> R[Relations]
      N --> P[Prose]
      H --> C1[Section Chunks]
      O --> C2[Observation Chunks]
      R --> C3[Relation Chunks]
      P --> C4[Merged Chunks ~900 chars]
      C1 --> E[Embeddings]
      C2 --> E
      C3 --> E
      C4 --> E
---
::

The chunking follows the note's structure:

- **Headers** create chunk boundaries — each section becomes its own chunk
- **Observations** (`- [category] value`) are indexed individually
- **Relations** (`- relation_type [[Target]]`) are indexed individually
- **Prose** paragraphs are merged into chunks of ~900 characters with ~120 character overlap at boundaries

This means a search for "water temperature for brewing" can surface the specific observation `- [technique] Water temperature at 205°F extracts optimal compounds` rather than returning the entire "Coffee Brewing Methods" note.

### Observation-level results

Vector and hybrid search return individual observations and relations as results, not just whole notes. This is different from keyword search, which returns entire entities.

Searching for "water temperature for brewing" in vector mode might return:

1. `[technique] Water temperature at 205°F extracts optimal compounds` from "Coffee Brewing Methods"
2. `[tip] Let water cool 30 seconds off boil for green tea` from "Tea Preparation Notes"
3. The entity "Coffee Brewing Methods" (because the overall note is also relevant)

You find the specific fact you need without reading through entire documents.

### Hybrid fusion

Hybrid mode runs text and vector search independently, then merges results using reciprocal rank fusion (RRF):

::mermaid
---
code: |
  flowchart LR
      Q[Query] --> FTS[Text Search]
      Q --> VS[Vector Search]
      FTS --> R1[Ranked Results]
      VS --> R2[Ranked Results]
      R1 --> RRF[Reciprocal Rank Fusion]
      R2 --> RRF
      RRF --> F[Final Ranking]
---
::

The fusion score for each result:

```
score = 1/(k + text_rank) + 1/(k + vector_rank)
```

where `k=60` is a smoothing constant. Items in both lists get scores from both terms. Items in only one list get a score from one term plus nothing from the other — so they rank lower.

A hybrid search for "authentication security" finds notes that:
- Contain those exact words *and* discuss related concepts (ranks highest — both signals agree)
- Discuss "auth hardening" or "login protection" (semantic match only)
- Mention "authentication" in passing (keyword match only)

### Deduplication

Each chunk has a content hash. When notes are re-synced or reindexed, unchanged chunks skip re-embedding. Only modified content triggers new embeddings. Editing one note in a thousand-note knowledge base only re-embeds the chunks that changed.

---

## Configuration

| Config field | Env var | Default | Description |
|---|---|---|---|
| `semantic_search_enabled` | `BASIC_MEMORY_SEMANTIC_SEARCH_ENABLED` | `true` | Enable semantic search |
| `semantic_embedding_provider` | `BASIC_MEMORY_SEMANTIC_EMBEDDING_PROVIDER` | `"fastembed"` | `"fastembed"` (local) or `"openai"` (API) |
| `semantic_embedding_model` | `BASIC_MEMORY_SEMANTIC_EMBEDDING_MODEL` | `"bge-small-en-v1.5"` | Embedding model identifier |
| `semantic_embedding_dimensions` | `BASIC_MEMORY_SEMANTIC_EMBEDDING_DIMENSIONS` | auto-detected | 384 (FastEmbed), 1536 (OpenAI) |
| `semantic_embedding_batch_size` | `BASIC_MEMORY_SEMANTIC_EMBEDDING_BATCH_SIZE` | `64` | Texts per embedding batch |
| `semantic_vector_k` | `BASIC_MEMORY_SEMANTIC_VECTOR_K` | `100` | Vector candidate count |
| `semantic_min_similarity` | `BASIC_MEMORY_SEMANTIC_MIN_SIMILARITY` | `0.55` | Minimum similarity threshold |

### Similarity threshold

The `semantic_min_similarity` setting controls which results are "similar enough" to return. The value ranges from `0.0` to `1.0`:

- **Higher** (e.g., `0.7`) — Fewer results, stronger relevance. Good for focused queries.
- **Lower** (e.g., `0.3`) — More results, looser associations. Good for exploration.
- **`0.0`** — Disables filtering. All vector results returned regardless of score.

The default `0.55` is a reasonable middle ground. Override per query:

```python
# Stricter — only close matches
search_notes(query="auth hardening", search_type="hybrid", min_similarity=0.7)

# Looser — cast a wider net
search_notes(query="auth hardening", search_type="hybrid", min_similarity=0.3)
```

See [Configuration](/reference/configuration) for the full config reference.

---

## Embedding providers

### FastEmbed (default)

- Runs locally — no API key, no network calls, no cost
- Model: `BAAI/bge-small-en-v1.5`
- Dimensions: 384

### OpenAI

- Requires an [OpenAI API subscription](https://platform.openai.com/api-keys) and `OPENAI_API_KEY` environment variable
- Model: `text-embedding-3-small`
- Dimensions: 1536
- Higher dimensions capture more nuance, which can improve results for large or domain-specific knowledge bases

Switching provider or model requires rebuilding embeddings:

```bash
bm reindex --embeddings
```

---

## Reindexing

```bash
# Rebuild both search index and embeddings
bm reindex

# Rebuild only vector embeddings
bm reindex --embeddings

# Rebuild only full-text search index
bm reindex --search

# Reindex a specific project
bm reindex -p my-project
```

### When you need to reindex

- **First time** — After installing semantic extras
- **Switching provider or model** — Embeddings from different models aren't compatible
- **After a database reset** — `bm reset` clears everything
- **Troubleshooting** — A fresh reindex can fix index issues

---

## Next steps

:::card-group
::card
---
title: MCP Tools Reference
icon: i-lucide-wrench
to: /reference/mcp-tools-reference
---
`search_notes` parameters and search modes.
::

::card
---
title: Configuration
icon: i-lucide-settings
to: /reference/configuration
---
All semantic settings and environment variables.
::

::card
---
title: Schema System
icon: i-lucide-shield-check
to: /concepts/schema-system
---
Define and validate note structure with schemas.
::
:::
